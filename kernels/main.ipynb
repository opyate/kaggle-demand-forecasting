{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38be8b2c0415b72970807ad9e951b50ebe8555cf"
   },
   "source": [
    "**Table of contents**\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Preparation](#Preparation)\n",
    "  * [Dependencies](#Dependencies)\n",
    "  * [Load the datasets](#Load-the-datasets)\n",
    "* [ARIMA](#ARIMA)\n",
    "* [Time series data exploration](#Time-series-data-exploration)\n",
    "  * [Distribution of sales](#Distribution-of-sales)\n",
    "  * [How does sales vary across stores](#How-does-sales-vary-across-stores)\n",
    "  * [How does sales vary across items](#How-does-sales-vary-across-items)\n",
    "  * [Time-series visualization of the sales](#Time-series-visualization-of-the-sales)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Kernel for the [demand forecasting](https://www.kaggle.com/c/demand-forecasting-kernels-only) Kaggle competition.\n",
    "\n",
    "Answer some of the questions posed:\n",
    "\n",
    "* What's the best way to deal with seasonality?\n",
    "* Should stores be modeled separately, or can you pool them together?\n",
    "* Does deep learning work better than ARIMA?\n",
    "* Can either beat xgboost?\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  # Preparation\n",
    "  \n",
    "  ##Â Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "155a10ac96338af61df8e5481ae98060276e9dfd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6985b0328326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58a45a9ff3fd02b31232abd5791eca57affd8a94"
   },
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b88da5611bebd0df7ac88b6659defcc262820563",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# First let us load the datasets into different Dataframes\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "sample_df = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "# Dimensions\n",
    "print('Train shape:', train_df.shape)\n",
    "# Set of features we have are: date, store, and item\n",
    "display(train_df.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a84690c95c5c0aae511c15440f0e7361f1e5232"
   },
   "source": [
    "# Time series data exploration\n",
    "\n",
    "(This portion was [forked](https://www.kaggle.com/danofer/getting-started-with-time-series-features).)\n",
    "\n",
    "The goal of this kernel is data exploration of a time-series sales data of store items.   \n",
    "The tools `pandas`, `matplotlib`  and, `plotly`  are used for slicing & dicing the data and visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b48bfa06a99d35bb56717fa986d3256e3f2d3119"
   },
   "source": [
    "## Distribution of sales\n",
    "Now let us understand how the sales varies across all the items in all the stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b69edd51e1a32ad0ca485d1bc15d00617c1b59e",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sales distribution across the train data\n",
    "sales_df = train_df.copy(deep=True)\n",
    "sales_df['sales_bins'] = pd.cut(sales_df.sales, [0, 50, 100, 150, 200, 250])\n",
    "print('Max sale:', sales_df.sales.max())\n",
    "print('Min sale:', sales_df.sales.min())\n",
    "print('Avg sale:', sales_df.sales.mean())\n",
    "print()\n",
    "\n",
    "# Total number of data points\n",
    "total_points = pd.value_counts(sales_df.sales_bins).sum()\n",
    "print('Sales bucket v/s Total percentage:')\n",
    "display(pd.value_counts(sales_df.sales_bins).apply(lambda s: (s/total_points)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "169207e3808f81d2c9f4130417b5bde34063b373",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us visualize the same\n",
    "pd.value_counts(sales_df.sales_bins).plot(kind='bar', title='Sales distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29f21269211e572cabe4b1045aaf6266a568f802"
   },
   "source": [
    "As we can see, almost 92% of sales are less than 100. Max, min and average sales are 231, 0 and 52.25 respectively.   \n",
    "So any prediction model has to deal with the skewness in the data appropriately. \n",
    "\n",
    "## How does sales vary across stores\n",
    "Let us get a overview of sales distribution in the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6659dfd63098c8b6b5eb4511034be43a705527d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us understand the sales data distribution across the stores\n",
    "store_df = train_df.copy()\n",
    "sales_pivoted_df = pd.pivot_table(store_df, index='store', values=['sales','date'], columns='item', aggfunc=np.mean)\n",
    "# Pivoted dataframe\n",
    "display(sales_pivoted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f7089515341e96adb62576ae64a489788775eff"
   },
   "source": [
    "This pivoted dataframe has average sales per each store per each item.  \n",
    "Let use this dataframe and produce some interesting visualizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f83879948cebbf4fefabdce306ff6ef3385aa0a5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us calculate the average sales of all the items by each store\n",
    "sales_across_store_df = sales_pivoted_df.copy()\n",
    "sales_across_store_df['avg_sale'] = sales_across_store_df.apply(lambda r: r.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "053de0c3fd5fc9cdac35a6aec18cc655217abc42",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot of average sales per store\n",
    "sales_store_data = go.Scatter(\n",
    "    y = sales_across_store_df.avg_sale.values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size = sales_across_store_df.avg_sale.values,\n",
    "        color = sales_across_store_df.avg_sale.values,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = sales_across_store_df.index.values\n",
    ")\n",
    "data = [sales_store_data]\n",
    "\n",
    "sales_store_layout = go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Scatter plot of avg sales per store',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Stores',\n",
    "        ticklen= 10,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Avg Sales',\n",
    "        ticklen= 10,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=sales_store_layout)\n",
    "py.iplot(fig,filename='scatter_sales_store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d318e77358d48e08ea61f9b56d9d7a6475e8c77",
    "collapsed": true
   },
   "source": [
    "From the visualization, it is clear that the stores with ID 2 and 8 have higher average sales than the remaining stores and is a clear indication that they are doing good money!\n",
    "\n",
    "Whereas store with ID 7 has very poor performance in terms of average sales.\n",
    "\n",
    "## How does sales vary across items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d043e12c5d444d9aa0d5f55d984f8f37f95d99ea",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let us calculate the average sales of each of the item across all the stores\n",
    "sales_across_item_df = sales_pivoted_df.copy()\n",
    "# Aggregate the sales per item and add it as a new row in the same dataframe\n",
    "sales_across_item_df.loc[11] = sales_across_item_df.apply(lambda r: r.mean(), axis=0)\n",
    "# Note the 11th index row, which is the average sale of each of the item across all the stores\n",
    "#display(sales_across_item_df.loc[11:])\n",
    "avg_sales_per_item_across_stores_df = pd.DataFrame(data=[[i+1,a] for i,a in enumerate(sales_across_item_df.loc[11:].values[0])], columns=['item', 'avg_sale'])\n",
    "# And finally, sort by avg sale\n",
    "avg_sales_per_item_across_stores_df.sort_values(by='avg_sale', ascending=False, inplace=True)\n",
    "# Display the top 10 rows\n",
    "display(avg_sales_per_item_across_stores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cabcd0b709c1d8f2d8941119b5c9456196357b21"
   },
   "source": [
    "Great! Let us visualize these average sales per item!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20ccd99509aab1f355db59df0924b681d8562c03",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_sales_per_item_across_stores_sorted = avg_sales_per_item_across_stores_df.avg_sale.values\n",
    "# Scatter plot of average sales per item\n",
    "sales_item_data = go.Bar(\n",
    "    x=[i for i in range(0, 50)],\n",
    "    y=avg_sales_per_item_across_stores_sorted,\n",
    "    marker=dict(\n",
    "        color=avg_sales_per_item_across_stores_sorted,\n",
    "        colorscale='Blackbody',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = avg_sales_per_item_across_stores_df.item.values\n",
    ")\n",
    "data = [sales_item_data]\n",
    "\n",
    "sales_item_layout = go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Scatter plot of avg sales per item',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Items',\n",
    "        ticklen= 55,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Avg Sales',\n",
    "        ticklen= 10,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=sales_item_layout)\n",
    "py.iplot(fig,filename='scatter_sales_item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39aa0cc4715d9a80ca537b91c0ed420a50e03a6c"
   },
   "source": [
    "Amazing! The sales is uniformly distributed across all the items.   \n",
    "Top items with highest average sale are 15, 28, 13, 18 and with least average sales are 5, 1, 41 and so on.\n",
    "\n",
    "## Time-series visualization of the sales\n",
    "Let us see how sales of a given item in a given store varies in a span of 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec5507252a10e0b03ce84015b8dccd1ce78b2822",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_item_df = train_df.copy()\n",
    "# First, let us filterout the required data\n",
    "store_id = 10   # Some store\n",
    "item_id = 40    # Some item\n",
    "print('Before filter:', store_item_df.shape)\n",
    "store_item_df = store_item_df[store_item_df.store == store_id]\n",
    "store_item_df = store_item_df[store_item_df.item == item_id]\n",
    "print('After filter:', store_item_df.shape)\n",
    "#display(store_item_df.head())\n",
    "\n",
    "# Let us plot this now\n",
    "store_item_ts_data = [go.Scatter(\n",
    "    x=store_item_df.date,\n",
    "    y=store_item_df.sales)]\n",
    "py.iplot(store_item_ts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48824374b884c59acd488f4c35d68c6b9b4486ed"
   },
   "source": [
    "Woww! Clearly there is a pattern here! Feel free to play around with different store and item IDs.   \n",
    "Almost all the items and store combination has this pattern!\n",
    "\n",
    "The sales go high in June, July and August months. The sales will be lowest in December, January and February months. That's something!!\n",
    "\n",
    "Let us make it more interesting. What if we aggregate the sales on a montly basis and compare different items and stores.   \n",
    "This should help us understand how different item sales behave at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1924731e6c60a3205ccfa559cc699a0a02327a01",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_store_item_df = train_df.copy()\n",
    "# First, let us filterout the required data\n",
    "store_ids = [1, 1, 1, 1]   # Some stores\n",
    "item_ids = [10, 20, 30, 40]    # Some items\n",
    "print('Before filter:', multi_store_item_df.shape)\n",
    "multi_store_item_df = multi_store_item_df[multi_store_item_df.store.isin(store_ids)]\n",
    "multi_store_item_df = multi_store_item_df[multi_store_item_df.item.isin(item_ids)]\n",
    "print('After filter:', multi_store_item_df.shape)\n",
    "#display(multi_store_item_df)\n",
    "# TODO Monthly avg sales\n",
    "\n",
    "# Let us plot this now\n",
    "multi_store_item_ts_data = []\n",
    "for st,it in zip(store_ids, item_ids):\n",
    "    flt = multi_store_item_df[multi_store_item_df.store == st]\n",
    "    flt = flt[flt.item == it]\n",
    "    multi_store_item_ts_data.append(go.Scatter(x=flt.date, y=flt.sales, name = \"Store:\" + str(st) + \",Item:\" + str(it)))\n",
    "py.iplot(multi_store_item_ts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a304efef2242bbda712ec06298984e7bd1dd901"
   },
   "source": [
    "Interesting!!   \n",
    "Though the pattern remains same across different stores and items combinations, the **actual sale value consitently varies with the same scale**. \n",
    "\n",
    "As we can see in the visualization, item 10 has consistently highest sales through out the span of 5 years!   \n",
    "This is an interesting behaviour that can be seen across almost all the items. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc8f5a920af25f40907e3cc2b674ea0ea5439383",
    "collapsed": true
   },
   "source": [
    "# ARIMA\n",
    "\n",
    "ARIMA is Autoregressive Integrated Moving Average Model, which is a component of SARIMAX, i.e. Seasonal ARIMA with eXogenous regressors.\n",
    "\n",
    "(sources: [1](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/), [2](https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3), [3](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases))\n",
    "\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04a2e7ceec8c4c84490228e96582d40236c9f150"
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20563f9eebae194c30bae3d8690abdeef534210c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "train_df['month'] = train_df['date'].dt.month\n",
    "train_df['day'] = train_df['date'].dt.dayofweek\n",
    "train_df['year'] = train_df['date'].dt.year\n",
    "\n",
    "test_df['month'] = test_df['date'].dt.month\n",
    "test_df['day'] = test_df['date'].dt.dayofweek\n",
    "test_df['year'] = test_df['date'].dt.year\n",
    "\n",
    "col = [i for i in test.columns if i not in ['date','id']]\n",
    "y = 'sales'\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(train_df[col],train_df[y], test_size=0.2, random_state=2018)\n",
    "\n",
    "def XGB_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=2017, num_rounds=500):\n",
    "    param = {}\n",
    "    param['objective'] = 'reg:linear'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['eval_metric'] = 'mae'\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "        \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "model = XGB_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "y_test = model.predict(xgb.DMatrix(test_df[col]), ntree_limit = model.best_ntree_limit)\n",
    "\n",
    "sample_df['sales'] = y_test\n",
    "sample_df.to_csv('xgb_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_kaggle)",
   "language": "python",
   "name": "conda_kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
